name: 📊 Calculate RS Ratings

on:
  workflow_dispatch:
  schedule:
    - cron: '20 20 * * 1-5'  # 8:20 UTC = 4:20 PM EST / 5:20 PM EDT

jobs:
  fetch-rs:
    strategy:
      matrix:
        partition: [0, 1, 2, 3]
    runs-on: ubuntu-latest
    name: fetch-rs-${{ matrix.partition }}
    steps:
      - name: Print workflow start time (EDT)
        run: |
          echo "Workflow started at $(date -u -d '4 hours ago' +'%I:%M %p EDT on %A, %B %d, %Y')"

      - name: ⬇️ Checkout repository
        uses: actions/checkout@v4

      - name: 🔍 Verify ticker_price.json
        run: |
          if [ ! -f data/ticker_price.json ]; then
            echo "Error: data/ticker_price.json not found"
            exit 1
          fi

      - name: 🗕️ Cache Python dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: 🔬 Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: 📦 Install dependencies
        run: |
          pip install --upgrade pip
          pip install yahooquery pandas numpy tqdm arcticdb

      - name: 🔢 Verify calculate_rs.py checksum
        run: |
          if [ ! -f scripts/calculate_rs.py ]; then
            echo "Error: scripts/calculate_rs.py not found"
            exit 1
          fi
          echo "SHA256: $(sha256sum scripts/calculate_rs.py)"

      - name: 🔎 Verify calculate_rs.py arguments
        run: |
          python scripts/calculate_rs.py -h

      - name: ▶️ Fetch data for partition ${{ matrix.partition }}
        run: |
          mkdir -p tmp/arctic_db
          mkdir -p logs
          rm -f logs/failed_tickers.log
          python scripts/calculate_rs.py data/ticker_price.json \
            --log-file logs/failed_tickers.log \
            --partition ${{ matrix.partition }} \
            --total-partitions 4

      - name: 📦 Upload ArcticDB data
        uses: actions/upload-artifact@v4
        with:
          name: arctic-db-${{ matrix.partition }}
          path: tmp/arctic_db/

      - name: 📊 Upload failed tickers log
        uses: actions/upload-artifact@v4
        with:
          name: failed-tickers-${{ matrix.partition }}
          path: logs/failed_tickers.log
          if-no-files-found: ignore

      - name: Print workflow end time (EDT)
        run: |
          echo "Workflow ended at $(date -u -d '4 hours ago' +'%I:%M %p EDT on %A, %B %d, %Y')"

  validate-arctic-data:
    needs:
      - fetch-rs
    runs-on: ubuntu-latest
    steps:
      - name: Print workflow start time (EDT)
        run: |
          echo "Workflow started at $(date -u -d '4 hours ago' +'%I:%M %p EDT on %A, %B %d, %Y')"

      - name: ⬇️ Checkout repo
        uses: actions/checkout@v4

      - name: 🗃️ Restore cached merged ArcticDB # 🆕
        uses: actions/cache@v4
        with:
          path: tmp/arctic_db
          key: arcticdb-merged-${{ github.run_id }}
          restore-keys: arcticdb-merged-

      - name: ⬇️ Download ArcticDB partitions
        uses: actions/download-artifact@v4
        with:
          path: tmp
          pattern: arctic-db-*

      - name: 🧪 Install ArcticDB
        run: |
          pip install arcticdb

      - name: 🪩 Merge ArcticDB partitions into one DB
        run: |
          echo "🔍 Checking for ArcticDB partition folders..."
          if ls tmp/arctic-db-* 1> /dev/null 2>&1; then
             echo "✅ ArcticDB partitions found:"
             find tmp -type d -name "arctic-db-*"
          else
             echo "❌ No ArcticDB partition folders (tmp/arctic-db-*) found"
             exit 1
          fi

          echo "🧹 Cleaning any previous merged DB at tmp/arctic_db..."
          rm -rf tmp/arctic_db
          mkdir -p tmp/arctic_db

          echo "🚀 Running ArcticDB merge script..."
          python3 scripts/merge_arcticdb.py --source-root tmp --dest-path tmp/arctic_db

          echo "📂 Listing merged DB contents:"
          ls -la tmp/arctic_db

          if [ -z "$(ls -A tmp/arctic_db 2>/dev/null)" ]; then
             echo "❌ Merge failed: tmp/arctic_db is empty"
             exit 1
          else
             echo "✅ ArcticDB merge completed successfully"
          fi

      - name: 🔮 Verify merged ArcticDB (Top 10 symbols)
        run: |
          pip install arcticdb pandas
          python <<EOF
          import arcticdb as adb
          import time

          start = time.time()
          try:
              arctic = adb.Arctic("lmdb://tmp/arctic_db")
              if not arctic.has_library("prices"):
                  raise ValueError("Library 'prices' not found.")

              lib = arctic.get_library("prices")
      
              print("🔎 Fetching symbols (this may take a few seconds)...")
              symbols = lib.list_symbols()
              elapsed = time.time() - start
      
              if not symbols:
                  raise ValueError("❌ No symbols found in 'prices' library.")
      
              print(f"✅ Found {len(symbols)} symbols in {elapsed:.2f} seconds")
              print("🕟 Sample symbols:", symbols[:10])
      
              # (Optional) Print min/max datetime for the first symbol
              sample = symbols[0]
              df = lib.read(sample).data
              print(f"📅 {sample} datetime range: {df['datetime'].min()} to {df['datetime'].max()}")
      
          except Exception as e:
              print(f"❌ ArcticDB error: {e}")
              exit(1)
          EOF

      - name: 📦 Upload merged ArcticDB
        uses: actions/upload-artifact@v4
        with:
          name: arctic-db-merged
          path: tmp/arctic_db

      - name: 🫼 Remove lock file
        run: |
          find tmp/arctic_db -name "lock.mdb" -exec rm -f {} \;

      - name: Print workflow end time (EDT)
        run: |
          echo "Workflow ended at $(date -u -d '4 hours ago' +'%I:%M %p EDT on %A, %B %d, %Y')"

  calculate-rs:
    needs: validate-arctic-data
    runs-on: ubuntu-latest
    steps:
      - name: Print workflow start time (EDT)
        run: |
          echo "Workflow started at $(date -u -d '4 hours ago' +'%I:%M %p EDT on %A, %B %d, %Y')"

      - name: ⬇️ Checkout repo
        uses: actions/checkout@v4

      - name: ⬇️ Download merged ArcticDB
        uses: actions/download-artifact@v4
        with:
          name: arctic-db-merged
          path: tmp/arctic_db

      - name: 📆 Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: 📦 Install dependencies
        run: |
          pip install --upgrade pip
          pip install pandas numpy tqdm arcticdb

      - name: 🔮 Verify ArcticDB before RS calculation
        run: |
          python <<EOF
          import arcticdb as adb
          try:
            arctic = adb.Arctic("lmdb://tmp/arctic_db")
            if not arctic.has_library("prices"):
                raise ValueError("Library 'prices' not found")
            print("\u2705 ArcticDB loaded successfully")
          except Exception as e:
            print(f"\u274c Failed to load ArcticDB: {e}")
            exit(0)
          EOF

      - name: 📃 Run RS calculation
        run: |
          mkdir -p RS_Logs RS_Data 
          python scripts/calculate_rs_from_db.py \
            --arctic-db-path tmp/arctic_db \
            --reference-ticker SPY \
            --output-dir RS_Data \
            --log-file RS_Logs/failed_tickers.log \
            --metadata-file data/ticker_price.json

      - name: 📃 Archive RS Results
        run: |
          mkdir -p archive
          current_date=$(date -u -d "4 hours ago" +"%m%d%Y")
          echo "📅 Archiving results for: ${current_date}"

          # Copy result CSVs to archive only
          for file in rs_stocks rs_industries RSRATING; do
            if [ -f "RS_Data/${file}.csv" ]; then
              cp RS_Data/${file}.csv archive/${file}_${current_date}.csv
              echo "✅ Archived ${file}.csv"
            else
              echo "❌ RS_Data/${file}.csv not found!"
              exit 1
            fi
          done

          # echo "🧹 Cleaning CSV files older than 90 days in archive..."
          # find archive -type f -name "*.csv" -mtime +90 -print -delete

          echo "📦 Final archived files:"
          ls -l archive | tail -n 10

      - name: 🗓️ Upload outputs
        uses: actions/upload-artifact@v4
        with:
          name: rs-results
          path: |
            RS_Data/*.csv
            RS_Logs/failed_tickers.log
            archive/*.csv

      - name: 📤 Commit and Push RS Results
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config --global user.name "GitHub Action"
          git config --global user.email "action@github.com"

          # 🔐 Authenticate using GitHub token
          git remote set-url origin https://x-access-token:${GITHUB_TOKEN}@github.com/${{ github.repository }}

          echo "📌 Repo status before commit:"
          git status

          # 📥 Save list of modified files
          mkdir -p RS_Logs
          git status | grep "modified:" > RS_Logs/git_modified_files.log || true
          echo "📝 Modified files saved to RS_Logs/git_modified_files.log"
          cat RS_Logs/git_modified_files.log

          echo "➕ Staging all relevant result files..."
          git add archive/*.csv RS_Data/*.csv

          # ✅ Skip commit if nothing is staged
          if git diff --cached --quiet; then
            echo "⚠️ No changes to commit. Skipping commit & push."
          else
            commit_date=$(date -u -d '4 hours ago' +'%m%d%Y')
            git commit -m "Add RS results for ${commit_date}"

            echo "🔄 Pulling latest changes from origin/main (rebase)..."
            git pull --rebase origin main

            echo "🚀 Pushing committed results to GitHub..."
            git push origin main
          fi

      - name: Print workflow end time (EDT)
        run: |
          echo "Workflow ended at $(date -u -d '4 hours ago' +'%I:%M %p EDT on %A, %B %d, %Y')"
